{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Learning Rate Warmup\n",
    "\n",
    "\n",
    "\n",
    "This exercise asks you to experiment with learning rate warmup during cosine annealing.\n",
    "\n",
    "Learning rate warmup is a technique that involves gradually increasing the learning rate from a small value to a larger target value over a certain number of iterations or epochs. Learning rate warmup has empirically been shown to stabilize training and improve convergence.\n",
    "\n",
    "In this notebook, we are adapting the [cosine annealing code from Unit 6.2 Part 5](https://github.com/Lightning-AI/dl-fundamentals/blob/main/unit06-dl-tips/6.2-learning-rates/6.2-part5-3-scheduler-cosine.ipynb). \n",
    "\n",
    "In particular, your task is to replace the `torch.optim.lr_scheduler.CosineAnnealingLR` scheduler with a similar scheduler that supports warmup. For this, we are going to use the \n",
    " `LinearWarmupCosineAnnealingLR` class from the PyTorch Lightning Bolts library, which can be installed via \n",
    "\n",
    "\n",
    "    pip install lightning-bolts\n",
    "\n",
    "And you can find more about the `LinearWarmupCosineAnnealingLR` usage in the [documentation here](https://pytorch-lightning-bolts.readthedocs.io/en/latest/schedulers/warmup_cosine_annealing.html)\n",
    "\n",
    "Note that you can use the accompanying notebook as a template and fill in the marked blanks. If you use a schedule similar to the one shown in the image below, you should get ~89% accuracy.\n",
    "\n",
    "![warmup](warmup.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
